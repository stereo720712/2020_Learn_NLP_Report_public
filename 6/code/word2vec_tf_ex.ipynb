{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0-dev20201101\n"
     ]
    }
   ],
   "source": [
    "#ã€€W2V-CBOW Model\n",
    "from __future__ import absolute_import , division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec:\n",
    "    def __init__(self, vocab_size=0, embedding_dim=16, optimizer='sgd', epochs=10000):\n",
    "        self.vocab_size=vocab_size\n",
    "        self.embedding_dim=5\n",
    "        self.epochs=epochs\n",
    "        if optimizer=='adam':\n",
    "            self.optimizer = tf.optimizers.Adam()\n",
    "        else:\n",
    "            self.optimizer = tf.optimizers.SGD(learning_rate=0.1)\n",
    "    # Training method takes 2 inputs x_train and y_train that are one-hot vectors. \n",
    "    #We continuously optimize the weight and bias using tf.GradientTape().\n",
    "    def train(self, x_train=None, y_train=None):\n",
    "        self.W1 = tf.Variable(tf.random.normal([self.vocab_size, self.embedding_dim]))\n",
    "        self.b1 = tf.Variable(tf.random.normal([self.embedding_dim]))\n",
    "        \n",
    "        self.W2 = tf.Variable(tf.random.normal([self.embedding_dim, self.vocab_size]))\n",
    "        self.b2 = tf.Variable(tf.random.normal([self.vocab_size]))\n",
    "        \n",
    "        for _ in range(self.epochs):\n",
    "            with tf.GradientTape() as t:\n",
    "                hidden_layer = tf.add(tf.matmul(x_train, self.W1), self.b1)\n",
    "                output_layer = tf.nn.softmax(tf.add(tf.matmul(hidden_layer, self.W2), self.b2))\n",
    "                cross_entropy_loss = tf.reduce_mean(-tf.math.reduce_sum(y_train * tf.math.log(\n",
    "                output_layer),axis=1))\n",
    "                grads = t.gradient(cross_entropy_loss,[self.W1, self.b1, self.W2, self.b2])\n",
    "                self.optimizer.apply_gradients(zip(grads, [self.W1, self.b1, self.W2, self.b2]))\n",
    "                if(_ % 100 == 0):\n",
    "                    print(cross_entropy_loss)\n",
    "                \n",
    "    def vectorized(self,word_idx):\n",
    "        return(self.W1 + self.b1)[word_idx]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ex\n",
    "corpus_raw = 'He is the king . The king is royal . She is the royal  queen '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['he', 'is', 'the', 'king'],\n",
       " ['the', 'king', 'is', 'royal'],\n",
       " ['she', 'is', 'the', 'royal', 'queen']]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#conver to lower case\n",
    "corpus_raw = corpus_raw.lower()\n",
    "raw_sentences = corpus_raw.split('.')\n",
    "sentences = []\n",
    "for sentence in raw_sentences:\n",
    "    sentences.append(sentence.split())\n",
    "sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['he', 'is'],\n",
       " ['he', 'the'],\n",
       " ['is', 'he'],\n",
       " ['is', 'the'],\n",
       " ['is', 'king'],\n",
       " ['the', 'he'],\n",
       " ['the', 'is'],\n",
       " ['the', 'king'],\n",
       " ['king', 'is'],\n",
       " ['king', 'the'],\n",
       " ['the', 'king'],\n",
       " ['the', 'is'],\n",
       " ['king', 'the'],\n",
       " ['king', 'is'],\n",
       " ['king', 'royal'],\n",
       " ['is', 'the'],\n",
       " ['is', 'king'],\n",
       " ['is', 'royal'],\n",
       " ['royal', 'king'],\n",
       " ['royal', 'is'],\n",
       " ['she', 'is'],\n",
       " ['she', 'the'],\n",
       " ['is', 'she'],\n",
       " ['is', 'the'],\n",
       " ['is', 'royal'],\n",
       " ['the', 'she'],\n",
       " ['the', 'is'],\n",
       " ['the', 'royal'],\n",
       " ['the', 'queen'],\n",
       " ['royal', 'is'],\n",
       " ['royal', 'the'],\n",
       " ['royal', 'queen'],\n",
       " ['queen', 'the'],\n",
       " ['queen', 'royal']]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "WINDOW_SIZE = 2\n",
    "for sentence in sentences:\n",
    "    for word_index, word in enumerate(sentence):\n",
    "        for nb_word in sentence[max(word_index - WINDOW_SIZE, 0): min(word_index + WINDOW_SIZE\n",
    "                               , len(sentence)) + 1]:\n",
    "            if nb_word != word:\n",
    "                data.append([word, nb_word])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For convenience, create 2 helper dictionaries, i.e. word2int and int2word.\n",
    "They do the simple mapping between words and corresponding integer values.\n",
    "'''\n",
    "words = []\n",
    "for word in corpus_raw.split():\n",
    "    if word != '.':\n",
    "        words.append(word)\n",
    "words = set(words) # remove repeat\n",
    "word2int = {}\n",
    "int2word={}\n",
    "vocab_size = len(words) # gives the total number of unique words\n",
    "for i, word in enumerate(words):\n",
    "    word2int[word] = i\n",
    "    int2word[i] = word\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode to x_train and y_train\n",
    "# function to convert numbers to one hot vectors\n",
    "def to_one_hot(data_point_index, vocab_size):\n",
    "    temp = np.zeros(vocab_size)\n",
    "    temp[data_point_index] = 1\n",
    "    return temp\n",
    "\n",
    "x_train = [] # input word\n",
    "y_train = [] # output word\n",
    "for data_word in data:\n",
    "    x_train.append(to_one_hot(word2int[data_word[0]],vocab_size))\n",
    "    y_train.append(to_one_hot(word2int[data_word[1]],vocab_size))\n",
    "\n",
    "# convert to numpy arrays\n",
    "x_train = np.asarray(x_train, dtype='float32')\n",
    "y_train = np.asarray(y_train, dtype='float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0455637, shape=(), dtype=float32)\n",
      "tf.Tensor(4.504267, shape=(), dtype=float32)\n",
      "tf.Tensor(3.5028133, shape=(), dtype=float32)\n",
      "tf.Tensor(2.9092138, shape=(), dtype=float32)\n",
      "tf.Tensor(2.523383, shape=(), dtype=float32)\n",
      "tf.Tensor(2.2378201, shape=(), dtype=float32)\n",
      "tf.Tensor(2.0203726, shape=(), dtype=float32)\n",
      "tf.Tensor(1.8567343, shape=(), dtype=float32)\n",
      "tf.Tensor(1.7347165, shape=(), dtype=float32)\n",
      "tf.Tensor(1.6440392, shape=(), dtype=float32)\n",
      "tf.Tensor(1.5762101, shape=(), dtype=float32)\n",
      "tf.Tensor(1.5245409, shape=(), dtype=float32)\n",
      "tf.Tensor(1.4843237, shape=(), dtype=float32)\n",
      "tf.Tensor(1.4526641, shape=(), dtype=float32)\n",
      "tf.Tensor(1.4278345, shape=(), dtype=float32)\n",
      "tf.Tensor(1.4084945, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3933729, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3813945, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3717698, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3639414, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3575104, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3521835, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3477412, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3440166, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3408805, shape=(), dtype=float32)\n",
      "tf.Tensor(1.338231, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3359861, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3340795, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3324555, shape=(), dtype=float32)\n",
      "tf.Tensor(1.331068, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3298783, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3288542, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3279691, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3272003, shape=(), dtype=float32)\n",
      "tf.Tensor(1.32653, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3259429, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3254263, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3249704, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3245659, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3242061, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3238847, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3235968, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3233383, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3231052, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3228947, shape=(), dtype=float32)\n",
      "tf.Tensor(1.322704, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3225311, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3223734, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3222299, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3220987, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3219786, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3218683, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3217672, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3216743, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3215888, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3215097, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3214369, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3213693, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3213068, shape=(), dtype=float32)\n",
      "tf.Tensor(1.321249, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3211954, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3211457, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3210993, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3210561, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3210161, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3209786, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3209438, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3209113, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3208809, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3208526, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3208262, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3208014, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3207781, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3207564, shape=(), dtype=float32)\n",
      "tf.Tensor(1.320736, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3207171, shape=(), dtype=float32)\n",
      "tf.Tensor(1.320699, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3206824, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3206664, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3206517, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3206378, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3206248, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3206125, shape=(), dtype=float32)\n",
      "tf.Tensor(1.320601, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3205901, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3205799, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3205702, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3205613, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3205526, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3205445, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3205371, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3205298, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3205233, shape=(), dtype=float32)\n",
      "tf.Tensor(1.320517, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3205109, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3205051, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3204999, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3204947, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3204901, shape=(), dtype=float32)\n",
      "tf.Tensor(1.3204855, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "w2v = Word2Vec(vocab_size=vocab_size, optimizer='adam', epochs=10000)\n",
    "w2v.train(x_train, y_train)\n",
    "# training process \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
       "array([-2.004537  ,  0.32331777, -0.6099627 , -0.5219996 , -3.464445  ],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.vectorized(word2int['queen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
