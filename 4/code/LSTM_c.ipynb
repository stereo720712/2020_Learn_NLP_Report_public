{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model ,Jay lyrics\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as f\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import zipfile\n",
    "import math\n",
    "sys.path.append(\"..\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "def load_data_jay_lyrics():\n",
    "    \"\"\"加载周杰伦歌词数据集\"\"\"\n",
    "    with zipfile.ZipFile('./data/jaychou_lyrics.txt.zip') as zin:\n",
    "        with zin.open('jaychou_lyrics.txt') as f:\n",
    "            corpus_chars = f.read().decode('utf-8')\n",
    "    corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    corpus_chars = corpus_chars[0:10000]\n",
    "    idx_to_char = list(set(corpus_chars))\n",
    "    char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n",
    "    vocab_size = len(char_to_idx)\n",
    "    corpus_indices = [char_to_idx[char] for char in corpus_chars]\n",
    "    return corpus_indices, char_to_idx, idx_to_char, vocab_size\n",
    "\n",
    "def data_iter_random(corpus_indices, batch_size, num_steps, ctx=None):\n",
    "    # 减1是因为输出的索引是相应输入的索引加1\n",
    "    num_examples = (len(corpus_indices) - 1) // num_steps\n",
    "    epoch_size = num_examples // batch_size\n",
    "    example_indices = list(range(num_examples))\n",
    "    random.shuffle(example_indices)\n",
    "\n",
    "    # 返回从pos开始的长为num_steps的序列\n",
    "    def _data(pos):\n",
    "        return corpus_indices[pos: pos + num_steps]\n",
    "\n",
    "    for i in range(epoch_size):\n",
    "        # 每次读取batch_size个随机样本\n",
    "        i = i * batch_size\n",
    "        batch_indices = example_indices[i: i + batch_size]\n",
    "        X = [_data(j * num_steps) for j in batch_indices]\n",
    "        Y = [_data(j * num_steps + 1) for j in batch_indices]\n",
    "        yield np.array(X, ctx), np.array(Y, ctx)\n",
    "\n",
    "def data_iter_random(corpus_indices, batch_size, num_steps, ctx=None):\n",
    "    # 减1是因为输出的索引是相应输入的索引加1\n",
    "    num_examples = (len(corpus_indices) - 1) // num_steps\n",
    "    epoch_size = num_examples // batch_size\n",
    "    example_indices = list(range(num_examples))\n",
    "    random.shuffle(example_indices)\n",
    "\n",
    "    # 返回从pos开始的长为num_steps的序列\n",
    "    def _data(pos):\n",
    "        return corpus_indices[pos: pos + num_steps]\n",
    "\n",
    "    for i in range(epoch_size):\n",
    "        # 每次读取batch_size个随机样本\n",
    "        i = i * batch_size\n",
    "        batch_indices = example_indices[i: i + batch_size]\n",
    "        X = [_data(j * num_steps) for j in batch_indices]\n",
    "        Y = [_data(j * num_steps + 1) for j in batch_indices]\n",
    "        yield np.array(X, ctx), np.array(Y, ctx)        \n",
    "        \n",
    "def data_iter_consecutive(corpus_indices, batch_size, num_steps, ctx=None):\n",
    "    corpus_indices = np.array(corpus_indices)\n",
    "    data_len = len(corpus_indices)\n",
    "    batch_len = data_len // batch_size\n",
    "    indices = corpus_indices[0: batch_size*batch_len].reshape((\n",
    "        batch_size, batch_len))\n",
    "    epoch_size = (batch_len - 1) // num_steps\n",
    "    \n",
    "    for i in range(epoch_size):\n",
    "        i = i * num_steps\n",
    "        X = indices[:, i: i + num_steps]\n",
    "        Y = indices[:, i + 1: i + num_steps + 1]\n",
    "        yield X, Y\n",
    "       \n",
    "        \n",
    "def to_onehot(X, size):  \n",
    "    # X shape: (batch), output shape: (batch, n_class)\n",
    "    return [tf.one_hot(x, size,dtype=tf.float32) for x in X.T]\n",
    "\n",
    "# rnn common function\n",
    "# 计算裁剪后的梯度\n",
    "def grad_clipping(grads,theta):\n",
    "    norm = np.array([0])\n",
    "    for i in range(len(grads)):\n",
    "        norm+=tf.math.reduce_sum(grads[i] ** 2)\n",
    "    #print(\"norm\",norm)\n",
    "    norm = np.sqrt(norm).item()\n",
    "    new_gradient=[]\n",
    "    if norm > theta:\n",
    "        for grad in grads:\n",
    "            new_gradient.append(grad * theta / norm)\n",
    "    else:\n",
    "        for grad in grads:\n",
    "            new_gradient.append(grad)  \n",
    "    #print(\"new_gradient\",new_gradient)\n",
    "    return new_gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_rnn(prefix, num_chars, rnn, params, init_rnn_state,\n",
    "                num_hiddens, vocab_size,idx_to_char, char_to_idx):\n",
    "    state = init_rnn_state(1, num_hiddens)\n",
    "    output = [char_to_idx[prefix[0]]]\n",
    "    for t in range(num_chars + len(prefix) - 1):\n",
    "        # 将上一时间步的输出作为当前时间步的输入\n",
    "        X = tf.convert_to_tensor(to_onehot(np.array([output[-1]]), vocab_size),dtype=tf.float32)\n",
    "        X = tf.reshape(X,[1,-1])\n",
    "        # 计算输出和更新隐藏状态\n",
    "        (Y, state) = rnn(X, state, params)\n",
    "        # 下一个时间步的输入是prefix里的字符或者当前的最佳预测字符\n",
    "        if t < len(prefix) - 1:\n",
    "            output.append(char_to_idx[prefix[t + 1]])\n",
    "        else:\n",
    "            output.append(int(np.array(tf.argmax(Y[0],axis=1))))\n",
    "    #print(output)\n",
    "    #print([idx_to_char[i] for i in output])\n",
    "    return ''.join([idx_to_char[i] for i in output])\n",
    "\n",
    "def train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\n",
    "                          vocab_size,  corpus_indices, idx_to_char,\n",
    "                          char_to_idx, is_random_iter, num_epochs, num_steps,\n",
    "                          lr, clipping_theta, batch_size, pred_period,\n",
    "                          pred_len, prefixes):\n",
    "    if is_random_iter:\n",
    "        data_iter_fn = data_iter_random\n",
    "    else:\n",
    "        data_iter_fn = data_iter_consecutive\n",
    "    params = get_params()\n",
    "    #loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        if not is_random_iter:  # 如使用相邻采样，在epoch开始时初始化隐藏状态\n",
    "            state = init_rnn_state(batch_size, num_hiddens)\n",
    "        l_sum, n, start = 0.0, 0, time.time()\n",
    "        data_iter = data_iter_fn(corpus_indices, batch_size, num_steps)\n",
    "        for X, Y in data_iter:\n",
    "            if is_random_iter:  # 如使用随机采样，在每个小批量更新前初始化隐藏状态\n",
    "                state = init_rnn_state(batch_size, num_hiddens)\n",
    "            #else:  # 否则需要使用detach函数从计算图分离隐藏状态\n",
    "                #for s in state:\n",
    "                    #s.detach()\n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "                tape.watch(params)\n",
    "                inputs = to_onehot(X, vocab_size)\n",
    "                # outputs有num_steps个形状为(batch_size, vocab_size)的矩阵\n",
    "                (outputs, state) = rnn(inputs, state, params)\n",
    "                # 拼接之后形状为(num_steps * batch_size, vocab_size)\n",
    "                outputs = tf.concat(outputs, 0)\n",
    "                # Y的形状是(batch_size, num_steps)，转置后再变成长度为\n",
    "                # batch * num_steps 的向量，这样跟输出的行一一对应\n",
    "                y = Y.T.reshape((-1,))\n",
    "                #print(Y,y)\n",
    "                y=tf.convert_to_tensor(y,dtype=tf.float32)\n",
    "                # 使用交叉熵损失计算平均分类误差\n",
    "                l = tf.reduce_mean(tf.losses.sparse_categorical_crossentropy(y,outputs))\n",
    "                #l = loss(y,outputs)\n",
    "                #print(\"loss\",np.array(l))\n",
    "                \n",
    "            grads = tape.gradient(l, params)\n",
    "            grads=grad_clipping(grads, clipping_theta)  # 裁剪梯度\n",
    "            optimizer.apply_gradients(zip(grads, params))\n",
    "            #sgd(params, lr, 1 , grads)  # 因为误差已经取过均值，梯度不用再做平均\n",
    "            l_sum += np.array(l).item() * len(y)\n",
    "            n += len(y)\n",
    "\n",
    "        if (epoch + 1) % pred_period == 0:\n",
    "            print('epoch %d, perplexity %f, time %.2f sec' % (\n",
    "                epoch + 1, math.exp(l_sum / n), time.time() - start))\n",
    "            #print(params)\n",
    "            for prefix in prefixes:\n",
    "                print(prefix)\n",
    "                print(' -', predict_rnn(\n",
    "                    prefix, pred_len, rnn, params, init_rnn_state,\n",
    "                    num_hiddens, vocab_size,  idx_to_char, char_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm model define\n",
    "# Lstm model weight default parameter\n",
    "def get_params():\n",
    "    def _one(shape):\n",
    "        return tf.Variable(tf.random.normal(shape=shape,stddev=0.01,mean=0,dtype=tf.float32))\n",
    "    def _three():\n",
    "        return (_one((num_inputs, num_hiddens)),\n",
    "                _one((num_hiddens, num_hiddens)),\n",
    "                tf.Variable(tf.zeros(num_hiddens), dtype=tf.float32))\n",
    "    W_xi, W_hi, b_i = _three()  # 输入门参数\n",
    "    W_xf, W_hf, b_f = _three()  # 遗忘门参数\n",
    "    W_xo, W_ho, b_o = _three()  # 输出门参数\n",
    "    W_xc, W_hc, b_c = _three()  # 候选记忆细胞参数\n",
    "\n",
    "    # 输出层参数\n",
    "    W_hq = _one((num_hiddens, num_outputs))\n",
    "    b_q = tf.Variable(tf.zeros(num_outputs), dtype=tf.float32)\n",
    "    return [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q]\n",
    "\n",
    "def init_lstm_state(batch_size, num_hiddens):\n",
    "    return (tf.zeros(shape=(batch_size, num_hiddens)), \n",
    "            tf.zeros(shape=(batch_size, num_hiddens)))\n",
    "\n",
    "# lstm model algorithm\n",
    "def lstm(inputs, state, params):\n",
    "    W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q = params\n",
    "    (H, C) = state\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        X=tf.reshape(X,[-1,W_xi.shape[0]])\n",
    "        I = tf.sigmoid(tf.matmul(X, W_xi) + tf.matmul(H, W_hi) + b_i)\n",
    "        F = tf.sigmoid(tf.matmul(X, W_xf) + tf.matmul(H, W_hf) + b_f)\n",
    "        O = tf.sigmoid(tf.matmul(X, W_xo) + tf.matmul(H, W_ho) + b_o)\n",
    "        C_tilda = tf.tanh(tf.matmul(X, W_xc) + tf.matmul(H, W_hc) + b_c)\n",
    "        C = F * C + I * C_tilda\n",
    "        H = O * tf.tanh(C)\n",
    "        Y = tf.matmul(H, W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "    return outputs, (H, C)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40, perplexity 17535.656662, time 1.55 sec\n",
      "分开\n",
      " - 分开我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我\n",
      "不分开\n",
      " - 不分开我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我\n",
      "epoch 80, perplexity 5054.733167, time 1.50 sec\n",
      "分开\n",
      " - 分开我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我\n",
      "不分开\n",
      " - 不分开我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我\n",
      "epoch 120, perplexity 2190.704095, time 1.52 sec\n",
      "分开\n",
      " - 分开我我说说说说说说我我说我说说说我说说我说说说我说说我说说说我说说我说说说我说说我说说说我说说我说说说\n",
      "不分开\n",
      " - 不分开我说说说说说说我说我说说我说说说我说说我说说我说说我说说我说说我说说我说说我说说我说说我说说我说说我\n",
      "epoch 160, perplexity 1086.014745, time 1.55 sec\n",
      "分开\n",
      " - 分开 我我我我我我我 你我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我\n",
      "不分开\n",
      " - 不分开 我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我我\n"
     ]
    }
   ],
   "source": [
    "# main\n",
    "\n",
    "# load data\n",
    "(corpus_indices, char_to_idx, idx_to_char, vocab_size) = load_data_jay_lyrics()\n",
    "\n",
    "# model config\n",
    "num_inputs, num_hiddens, num_outputs = vocab_size, 256, vocab_size\n",
    "\n",
    "# default parameter\n",
    "num_epochs, num_steps, batch_size, lr, clipping_theta = 160, 35, 32, 1e2, 1e-2\n",
    "pred_period, pred_len, prefixes = 40, 50, ['分开', '不分开']\n",
    "\n",
    "train_and_predict_rnn(lstm, get_params, init_lstm_state, num_hiddens,\n",
    "                          vocab_size, corpus_indices, idx_to_char,\n",
    "                          char_to_idx, False, num_epochs, num_steps, lr,\n",
    "                          clipping_theta, batch_size, pred_period, pred_len,\n",
    "                          prefixes)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
