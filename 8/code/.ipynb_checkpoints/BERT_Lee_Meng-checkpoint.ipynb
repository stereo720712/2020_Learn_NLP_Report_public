{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref\n",
    "# https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROXY\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "import fox_proxy\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from IPython.display import clear_output\n",
    "\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-chinese\"  # 指定繁簡中文 BERT-BASE 預訓練模型\n",
    "\n",
    "# 取得此預訓練模型所使用的 tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "\n",
    "clear_output()\n",
    "print(\"PyTorch 版本：\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT Constant\n",
    "_CLS = '[CLS]'\n",
    "_SEP = '[SEP]'\n",
    "_UNK = '[UNK]'\n",
    "_PAD = '[PAD]'\n",
    "_MASK = '[MASK]'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary in tokenizer\n",
    "vocab = tokenizer.vocab\n",
    "print('字典大小: ', len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 瞧瞧中文 BERT 字典裡頭紀錄的一些 tokens 以及其對應的索引\n",
    "import random\n",
    "random_tokens = random.sample(list(vocab),10)\n",
    "random_ids = [vocab[t] for t in random_tokens]\n",
    "\n",
    "print(\"{0:20}{1:15}\".format('token', 'index'))\n",
    "print(\"-\" * 25)\n",
    "for t, id in zip(random_tokens, random_ids):\n",
    "    print(\"{0:20}{1:15}\".format(t, id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ㄅㄆㄇㄈ\n",
    "indices = list(range(647,657))\n",
    "some_pairs = [(t,idx) for t, idx in vocab.items() if idx in indices ]\n",
    "for pair in some_pairs:\n",
    "    print(pair)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讓我們利用中文 BERT 的 tokenizer 將一個中文句子斷詞看看：\n",
    "text = \"[CLS] 等到潮水 [MASK] 了，就知道誰沒穿褲子。\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(text)\n",
    "print(tokens[:10],'...')\n",
    "print(ids[:10], '...')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "這段程式碼載入已經訓練好的 masked 語言模型並對有 [MASK] 的句子做預測\n",
    "\"\"\"\n",
    "from transformers import BertForMaskedLM\n",
    "\n",
    "# 除了tokens 以外我們還需要辨別句子的segment ids\n",
    "tokens_tensor = torch.tensor([ids]) #(1, seq_len)\n",
    "segments_tensors = torch.zeros_like(tokens_tensor) #(1, seq_len)\n",
    "maskedLM_model = BertForMaskedLM.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "# clear_output()\n",
    "\n",
    "# 使用  masked LM 估計 [MASK] 位置所代表的實際 token\n",
    "maskedLM_model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = maskedLM_model(tokens_tensor, segments_tensors)\n",
    "    predictions = outputs[0]\n",
    "    #(1, seq_len, num_hidden_units)\n",
    "\n",
    "del maskedLM_model\n",
    "\n",
    "# 將 [MASK] 位置的機率分布取 top k 最有可能的 tokens 出來\n",
    "masked_index = 5\n",
    "k = 3\n",
    "\n",
    "probs, indices = torch.topk(torch.softmax(predictions[0, masked_index], -1), k)\n",
    "predicted_tokens = tokenizer.convert_ids_to_tokens(indices.tolist())\n",
    "\n",
    "#顯示 top k 可能的字 , 一般我們取top 1 當預測值\n",
    "print('輸入 tokens : ', tokens[:10], '...' )\n",
    "print('-' * 50)\n",
    "for i,(t, p) in enumerate(zip(predicted_tokens, probs),1):\n",
    "    tokens[masked_index] = t\n",
    "    print('Top {} ({:2}%):{}'.format(i,int(p.item() * 100), tokens[:10]),'...')\n",
    "          \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT 的視覺化工具 BertViz\n",
    "# import sys\n",
    "# !test -d bertviz_repo || git clone https://github.com/jessevig/bertviz bertviz_repo\n",
    "# if not 'bertviz_repo' in sys.path:\n",
    "#   sys.path += ['bertviz_repo']\n",
    "sys.path.append('./bertviz_repo')\n",
    "# import packages\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from bertviz import head_view\n",
    "\n",
    "# 在 jupyter notebook 裡頭顯示 visualzation 的 helper\n",
    "def call_html():\n",
    "  import IPython\n",
    "  display(IPython.core.display.HTML('''\n",
    "        <script src=\"/static/components/requirejs/require.js\"></script>\n",
    "        <script>\n",
    "          requirejs.config({\n",
    "            paths: {\n",
    "              base: '/static/base',\n",
    "              \"d3\": \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.8/d3.min\",\n",
    "              jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min',\n",
    "            },\n",
    "          });\n",
    "        </script>\n",
    "        '''))\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "model_version = 'bert-base-chinese'\n",
    "model = BertModel.from_pretrained(model_version, output_attentions=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_version)\n",
    "\n",
    "# 情境1\n",
    "sentence_a = '胖虎叫大雄去買漫畫，'\n",
    "sentence_b = '回來慢了就打他。'\n",
    "\n",
    "# 得到tokens 後丟入 bert 取得 attention\n",
    "inputs = tokenizer.encode_plus(sentence_a, sentence_b, return_tensors='pt', add_sprcial_tokens=True)\n",
    "token_type_ids = inputs['token_type_ids']\n",
    "input_ids = inputs['input_ids']\n",
    "attention = model(input_ids, token_type_ids=token_type_ids)[-1]\n",
    "input_id_list = input_ids[0].tolist() # Batch index 0\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_id_list)\n",
    "call_html()\n",
    "head_view(attention, tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---\n",
    "\n",
    "# test txt classify\n",
    "\n",
    "# ---\n",
    "\n",
    "    1.準備原始文本數據\n",
    "    2.將原始文本轉換成 BERT 相容的輸入格式\n",
    "    3.在 BERT 之上加入新 layer 成下游任務模型\n",
    "    4.訓練該下游任務模型\n",
    "    5.對新樣本做推論\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/fake-news-pair-classification-challenge/data\n",
    "# =============================1. prepare original txt data\n",
    "import glob\n",
    "glob.glob(\"*.csv.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "前處理原始的訓練數據集。\n",
    "你不需了解細節，只需要看註解了解邏輯或是輸出的數據格式即可\n",
    "\"\"\"\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "#unzip train file\n",
    "#os.system('unzip train.csv.zip')\n",
    "\n",
    "# data process remove empty title examples\n",
    "df_train = pd.read_csv('train.csv')\n",
    "empty_title = ((df_train['title2_zh'].isnull()) \n",
    "              | (df_train['title1_zh'].isnull())  \n",
    "              | (df_train['title2_zh'] == '') \n",
    "              | (df_train['title2_zh'] == '0'))\n",
    "df_train = df_train[~empty_title]\n",
    "\n",
    "# 剔除過長樣本 避免 BERT 無法將整個輸入序列 放入記憶體不多的 CPU\n",
    "\n",
    "MAX_LENGTH = 30\n",
    "df_train = df_train[~empty_title]\n",
    "\n",
    "#剔除 過長樣本 避免 bert 無法將整個輸入序列 放入記憶體不多的gpu\n",
    "MAX_LENGTH  = 30\n",
    "df_train = df_train[~(df_train.title1_zh.apply(lambda x: len(x)) > MAX_LENGTH)]\n",
    "df_train = df_train[~(df_train.title2_zh.apply(lambda x: len(x)) > MAX_LENGTH)]\n",
    "\n",
    "# 只用 1% 的訓練數據 看看 BERT 對少量標註數據有多少幫助\n",
    "SAMPLE_FRAC = 0.01\n",
    "df_train = df_train.sample(frac=SAMPLE_FRAC, random_state=9527)\n",
    "\n",
    "# 去除不必要的欄位並重新命名兩標題的欄位名\n",
    "df_train = df_train.reset_index()\n",
    "df_train = df_train.loc[:, ['title1_zh','title2_zh', 'label']]\n",
    "df_train.columns = ['text_a', 'text_b', 'label']\n",
    "\n",
    "# idempotence 冪等 , 將處理結果 另存成 csv 供 Pytorch 使用\n",
    "df_train.to_csv('train2.csv', encoding='utf_8_sig', index=False)\n",
    "print('訓練樣本數:', len(df_train))\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.label.value_counts() / len(df_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('test.csv')\n",
    "df_test = df_test.loc[:, ['title1_zh', 'title2_zh', 'id']]\n",
    "df_test.columns = ['text_a', 'text_b', 'id']\n",
    "df_test.to_csv('test2.csv',  encoding='utf_8_sig', index=False)\n",
    "print('預測樣本數: ', len(df_test))\n",
    "df_test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = len(df_test) / len(df_train)\n",
    "print('測試集樣本數 / 訓練集樣本數 = {:.1f}倍'.format(ratio))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 將文本轉成 BBRT 相容的輸入格式\n",
    "\"\"\"\n",
    "實作一個可以用來讀取訓練 / 測試集的 Dataset，這是你需要徹底了解的部分。\n",
    "此 Dataset 每次將 tsv 裡的一筆成對句子轉換成 BERT 相容的格式，並回傳 3 個 tensors：\n",
    "- tokens_tensor：兩個句子合併後的索引序列，包含 [CLS] 與 [SEP]\n",
    "- segments_tensor：可以用來識別兩個句子界限的 binary tensor\n",
    "- label_tensor：將分類標籤轉換成類別索引的 tensor, 如果是測試集則回傳 None\n",
    "\"\"\"\n",
    "from torch.utils.data import Dataset\n",
    "import pysnooper\n",
    "\n",
    "# class FakeNewsDataset(Dataset):\n",
    "#     # 讀取之前處理後的csv 並初始化參數\n",
    "#     def __init__(self, mode, tokenizer):\n",
    "#         assert mode in ['train', 'test']\n",
    "#         self.mode = mode\n",
    "#         #大數據你會需要用 iterator = True\n",
    "#         self.df = pd.read_csv(mode + '2.csv', encoding='utf_8_sig')\n",
    "#         self.len = len(self.df)\n",
    "#         self.label_map = {'agreed': 0, 'disagreed':1, 'unrelated': 2}\n",
    "#         self.tokenizer = tokenizer # Bert tokenizer\n",
    "    \n",
    "#     # 回傳一筆訓練 / 測試數據\n",
    "   \n",
    "#     def __getitem__(self, idx):\n",
    "#         if self.mode =='test':\n",
    "#             text_a, text_b = self.df.iloc[idx, :2].values\n",
    "#             label_tensor = None\n",
    "#         else:\n",
    "#             text_a, text_b, label = self.df.iloc[idx,:].values\n",
    "#             # 將label 文字 轉換成索引方便的tensor\n",
    "#             label_id = self.label_map[label]\n",
    "#             label_tensor = torch.tensor(label_id)\n",
    "        \n",
    "#         #建立第一個句子的BERT tokens 並加入分隔符號[SEP]\n",
    "#         word_pieces = [_CLS]\n",
    "#         token_a = self.tokenizer.tokenize(text_a)\n",
    "#         word_pieces += token_a + [_SEP]\n",
    "#         len_a = len(word_pieces) \n",
    "        \n",
    "#         # 第二個句子 BERT tokens\n",
    "#         tokens_b = self.tokenizer.tokenize(text_b)\n",
    "#         word_pieces += tokens_b + [_SEP]\n",
    "#         len_b = len(word_pieces) - len_a \n",
    "        \n",
    "#         # 將整個 token 序列 轉成 索引 序列\n",
    "#         ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "#         tokens_tensor = torch.tensor(ids)\n",
    "        \n",
    "#         # 將第一句話包含 [SEP]的 token位置設為0, 其他為1 表示第二句\n",
    "#         segments_tensor= torch.tensor([0] * len_a + [1] * len_b,dtype=torch.long)\n",
    "        \n",
    "#         return (tokens_tensor, segments_tensor, label_tensor)\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return self.len\n",
    "    \n",
    "# # 初始化一個專門讀取訓練樣本的 Dataset 使用中文 BERT斷詞\n",
    "# trainset = FakeNewsDataset('train',tokenizer=tokenizer)\n",
    " \n",
    "    \n",
    "class FakeNewsDataset(Dataset):\n",
    "    # 讀取前處理後的 tsv 檔並初始化一些參數\n",
    "    def __init__(self, mode, tokenizer):\n",
    "        assert mode in [\"train\", \"test\"]  # 一般訓練你會需要 dev set\n",
    "        self.mode = mode\n",
    "        # 大數據你會需要用 iterator=True\n",
    "        self.df = pd.read_csv(mode + \"2.csv\", encoding='utf_8_sig').fillna(\"\")\n",
    "        self.len = len(self.df)\n",
    "        self.label_map = {'agreed': 0, 'disagreed': 1, 'unrelated': 2}\n",
    "        self.tokenizer = tokenizer  # 我們將使用 BERT tokenizer\n",
    "    \n",
    "    # 定義回傳一筆訓練 / 測試數據的函式\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == \"test\":\n",
    "            text_a, text_b = self.df.iloc[idx, :2].values\n",
    "            label_tensor = None\n",
    "        else:\n",
    "            text_a, text_b, label = self.df.iloc[idx, :].values\n",
    "            # 將 label 文字也轉換成索引方便轉換成 tensor\n",
    "            label_id = self.label_map[label]\n",
    "            label_tensor = torch.tensor(label_id)\n",
    "            \n",
    "        # 建立第一個句子的 BERT tokens 並加入分隔符號 [SEP]\n",
    "        word_pieces = [\"[CLS]\"]\n",
    "        tokens_a = self.tokenizer.tokenize(text_a)\n",
    "        word_pieces += tokens_a + [\"[SEP]\"]\n",
    "        len_a = len(word_pieces)\n",
    "        \n",
    "        # 第二個句子的 BERT tokens\n",
    "        tokens_b = self.tokenizer.tokenize(text_b)\n",
    "        word_pieces += tokens_b + [\"[SEP]\"]\n",
    "        len_b = len(word_pieces) - len_a\n",
    "        \n",
    "        # 將整個 token 序列轉換成索引序列\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "        \n",
    "        # 將第一句包含 [SEP] 的 token 位置設為 0，其他為 1 表示第二句\n",
    "        segments_tensor = torch.tensor([0] * len_a + [1] * len_b, \n",
    "                                        dtype=torch.long)\n",
    "        \n",
    "        return (tokens_tensor, segments_tensor, label_tensor)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    \n",
    "# 初始化一個專門讀取訓練樣本的 Dataset，使用中文 BERT 斷詞\n",
    "trainset = FakeNewsDataset(\"train\", tokenizer=tokenizer)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataset \n",
    "# 第一個樣本\n",
    "sample_idx = 0\n",
    "\n",
    "#將原始樣本拿出比較\n",
    "text_a, text_b, label = trainset.df.iloc[sample_idx].values\n",
    "\n",
    "# 利用剛剛建立的Dataset 取出轉換的 id_tensors\n",
    "tokens_tensor, segments_tensor, label_tensor = trainset[sample_idx]\n",
    "\n",
    "#將 token 還原成文本\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokens_tensor.tolist())\n",
    "combined_text = ''.join(tokens)\n",
    "\n",
    "print(f\"\"\"[原始文本]\n",
    "句子1:{text_a}\n",
    "句子2:{text_b}\n",
    "分類:{label}\n",
    "-------------------\n",
    "[Dataset 回傳的 tensors]\n",
    "token_tensor: {segments_tensor}\n",
    "\n",
    "segments_tensor:{segments_tensor}\n",
    "\n",
    "label_tensor   :{label_tensor}\n",
    "\n",
    "----------------------------\n",
    "\n",
    "[還原 token_tensors]\n",
    "{combined_text}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "實作可以一次回傳一個 mini-batch 的 DataLoader\n",
    "這個 DataLoader 吃我們上面定義的 `FakeNewsDataset`，\n",
    "回傳訓練 BERT 時會需要的 4 個 tensors：\n",
    "- tokens_tensors  : (batch_size, max_seq_len_in_batch)\n",
    "- segments_tensors: (batch_size, max_seq_len_in_batch)\n",
    "- masks_tensors   : (batch_size, max_seq_len_in_batch)\n",
    "- label_ids       : (batch_size)\n",
    "\"\"\"\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# 這個函式的輸入 `samples` 是一個 list，裡頭的每個 element 都是\n",
    "# 剛剛定義的 `FakeNewsDataset` 回傳的一個樣本，每個樣本都包含 3 tensors：\n",
    "# - tokens_tensor\n",
    "# - segments_tensor\n",
    "# - label_tensor\n",
    "# 它會對前兩個 tensors 作 zero padding，並產生前面說明過的 masks_tensors\n",
    "\n",
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    segments_tensors = [s[1] for s in samples]\n",
    "    \n",
    "    # 測試集有 labels\n",
    "    if samples[0][2] is not None:\n",
    "        label_ids = torch.stack([s[2] for s in samples])\n",
    "    else:\n",
    "        label_ids = None\n",
    "    \n",
    "    # zero pad 到同一序列長度\n",
    "    tokens_tensors = pad_sequence(tokens_tensors, \n",
    "                                  batch_first=True)\n",
    "    segments_tensors = pad_sequence(segments_tensors, \n",
    "                                    batch_first=True)\n",
    "    \n",
    "    # attention masks，將 tokens_tensors 裡頭不為 zero padding\n",
    "    # 的位置設為 1 讓 BERT 只關注這些位置的 tokens\n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape, \n",
    "                                dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(\n",
    "        tokens_tensors != 0, 1)\n",
    "    \n",
    "    return tokens_tensors, segments_tensors, masks_tensors, label_ids\n",
    "\n",
    "\n",
    "# 初始化一個每次回傳 64 個訓練樣本的 DataLoader\n",
    "# 利用 `collate_fn` 將 list of samples 合併成一個 mini-batch 是關鍵\n",
    "BATCH_SIZE = 32\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, \n",
    "                         collate_fn=create_mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "data = next(iter(trainloader))\n",
    "tokens_tensors, segments_tensors, masks_tensors, label_ids = data\n",
    "\n",
    "\n",
    "print(f\"\"\"\n",
    "tokens_tensors.shape   = {tokens_tensors.shape} \n",
    "{tokens_tensors}\n",
    "------------------------\n",
    "segments_tensors.shape = {segments_tensors.shape}\n",
    "{segments_tensors}\n",
    "------------------------\n",
    "masks_tensors.shape    = {masks_tensors.shape}\n",
    "{masks_tensors}\n",
    "------------------------\n",
    "label_ids.shape        = {label_ids.shape}\n",
    "{label_ids}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 在 BERT 之上加入新 layer 成下游任務模型\n",
    "# 載入一個可以做中文多分類任務的模型，n_class = 3\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-chinese\"\n",
    "NUM_LABELS = 3\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "# high-level 顯示此模型裡的 modules\n",
    "print(\"\"\"\n",
    "name            module\n",
    "----------------------\"\"\")\n",
    "for name, module in model.named_children():\n",
    "    if name == \"bert\":\n",
    "        for n, _ in module.named_children():\n",
    "            print(f\"{name}:{n}\")\n",
    "    else:\n",
    "        print(\"{:15} {}\".format(name, module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_transformers.modeling_bert import BertPreTrainedModel\n",
    "class BertForSequenceClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_labels=2):\n",
    "        super(BertForSequenceClassification, self).__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel(config)  # 載入預訓練 BERT\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        # 簡單 linear 層\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        # BERT 輸入就是 tokens, segments, masks\n",
    "        outputs = self.bert(input_ids, token_type_ids, attention_mask, ...)\n",
    "        ...\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        # 線性分類器將 dropout 後的 BERT repr. 轉成類別 logits\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        # 輸入有 labels 的話直接計算 Cross Entropy 回傳，方便！\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            return loss\n",
    "        # 有要求回傳注意矩陣的話回傳\n",
    "        elif self.output_attentions:\n",
    "            return all_attentions, logits\n",
    "        # 回傳各類別的 logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "定義一個可以針對特定 DataLoader 取得模型預測結果以及分類準確度的函式\n",
    "之後也可以用來生成上傳到 Kaggle 競賽的預測結果\n",
    "\n",
    "2019/11/22 更新：在將 `tokens`、`segments_tensors` 等 tensors\n",
    "丟入模型時，強力建議指定每個 tensor 對應的參數名稱，以避免 HuggingFace\n",
    "更新 repo 程式碼並改變參數順序時影響到我們的結果。\n",
    "\"\"\"\n",
    "\n",
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "      \n",
    "    with torch.no_grad():\n",
    "        # 遍巡整個資料集\n",
    "        for data in dataloader:\n",
    "            # 將所有 tensors 移到 GPU 上\n",
    "            if next(model.parameters()).is_cuda:\n",
    "                data = [t.to(\"cuda:0\") for t in data if t is not None]\n",
    "            \n",
    "            \n",
    "            # 別忘記前 3 個 tensors 分別為 tokens, segments 以及 masks\n",
    "            # 且強烈建議在將這些 tensors 丟入 `model` 時指定對應的參數名稱\n",
    "            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors)\n",
    "            \n",
    "            logits = outputs[0]\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "            \n",
    "            # 用來計算訓練集的分類準確率\n",
    "            if compute_acc:\n",
    "                labels = data[3]\n",
    "                total += labels.size(0)\n",
    "                correct += (pred == labels).sum().item()\n",
    "                \n",
    "            # 將當前 batch 記錄下來\n",
    "            if predictions is None:\n",
    "                predictions = pred\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, pred))\n",
    "    \n",
    "    if compute_acc:\n",
    "        acc = correct / total\n",
    "        return predictions, acc\n",
    "    return predictions\n",
    "    \n",
    "# 讓模型跑在 GPU 上並取得訓練集的分類準確率\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "model = model.to(device)\n",
    "_, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "print(\"classification acc:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 算算整個分類模型以及裡頭的簡單分類器有多少參數：\n",
    "def get_learnable_params(module):\n",
    "    return [p for p in module.parameters() if p.requires_grad]\n",
    "     \n",
    "model_params = get_learnable_params(model)\n",
    "clf_params = get_learnable_params(model.classifier)\n",
    "\n",
    "print(f\"\"\"\n",
    "整個分類模型的參數量：{sum(p.numel() for p in model_params)}\n",
    "線性分類器的參數量：{sum(p.numel() for p in clf_params)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 訓練該下游任務模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 訓練模式\n",
    "model.train()\n",
    "\n",
    "# 使用 Adam Optim 更新整個分類模型的參數\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "EPOCHS = 1  # 幸運數字\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for data in trainloader:\n",
    "        \n",
    "        tokens_tensors, segments_tensors, \\\n",
    "        masks_tensors, labels = [t.to(device) for t in data]\n",
    "\n",
    "        # 將參數梯度歸零\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(input_ids=tokens_tensors, \n",
    "                        token_type_ids=segments_tensors, \n",
    "                        attention_mask=masks_tensors, \n",
    "                        labels=labels)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # 紀錄當前 batch loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    # 計算分類準確率\n",
    "    _, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "\n",
    "    print('[epoch %d] loss: %.3f, acc: %.3f' %\n",
    "          (epoch + 1, running_loss, acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 對新樣本做推論\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 建立測試集。這邊我們可以用跟訓練時不同的 batch_size，看你 GPU 多大\n",
    "testset = FakeNewsDataset(\"test\", tokenizer=tokenizer)\n",
    "testloader = DataLoader(testset, batch_size=64, \n",
    "                        collate_fn=create_mini_batch)\n",
    "\n",
    "trst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 用分類模型預測測試集\n",
    "predictions = get_predictions(model, testloader)\n",
    "\n",
    "# 用來將預測的 label id 轉回 label 文字\n",
    "index_map = {v: k for k, v in testset.label_map.items()}\n",
    "\n",
    "# 生成 Kaggle 繳交檔案\n",
    "df = pd.DataFrame({\"Category\": predictions.tolist()})\n",
    "df['Category'] = df.Category.apply(lambda x: index_map[x])\n",
    "df_pred = pd.concat([testset.df.loc[:, [\"Id\"]], \n",
    "                          df.loc[:, 'Category']], axis=1)\n",
    "df_pred.to_csv('bert_1_prec_training_samples.csv', index=False)\n",
    "df_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 370.85,
   "position": {
    "height": "40px",
    "left": "1045px",
    "right": "20px",
    "top": "120px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
